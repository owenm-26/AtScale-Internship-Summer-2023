{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fa98f5d-d69f-4efa-b190-d2c56d7dc454",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![](/files/images/dbxatscale.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7af6319e-1469-4053-abf7-956faedf9c3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Overview\n",
    "Now that we have our model training on a schedule, we want to use it on a schedule. In this notebook we will classify with our model and write our classifications to a new Databricks table. First we need to establish our AtScale connection, so we will use XX_Establish_AtScale_Connection to connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f0ec58-5205-45a5-82d8-0fc97314c08b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./XX_Establish_AtScale_Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb36939c-11a2-416d-b180-98d660fc661c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next we need to read our model so we can use it for classification. MLFlow makes this process super easy, all we need to do is specify the name, a path to our model, and load it using mlflow.pyfunc.load_model. Then we can print our model out to make sure we have the successfully loaded our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f8131e-c9ef-4dae-aeb7-3d6cd926ece9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow.pyfunc.loaded_model:\n  artifact_path: model\n  flavor: mlflow.sklearn\n  run_id: bdef39ee21b04ddbab4cf667b270e4f3\n\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "model_name = \"Databricks_AI_Link_M5_Demo\"\n",
    "\n",
    "model_uri=f\"models:/{model_name}/latest\"\n",
    "model = mlflow.pyfunc.load_model(model_uri=model_uri)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c84c47-1db4-4127-b7ed-bf7392aa144f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now let's load our data that we are going to classify on. We will do this by generating a spark dataframe from our query which was initialized in our XX_Establish_AtScale_Connection, then we will convert the spark dataframe to pandas and use pandas' drop function to drop out our item column. In the realistic use case of this model we don't know what item the row represents, so let's drop out item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e506ef86-24ab-412e-99df-819642931b9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(query).toPandas().drop([\"item\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ee6ed4-08ca-454d-acf7-6fb8967cc92d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now predicting with our model is as easy as setting the value of a new column equal to the output of model.predict for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc61914d-954f-4fc1-9a1b-c407d1dca2d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l2 is set=0.176191006422464, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.176191006422464\n[LightGBM] [Warning] lambda_l1 is set=0.2347409701792684, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2347409701792684\n"
     ]
    }
   ],
   "source": [
    "df[\"predicted_class\"] = model.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "840d4537-886a-48ba-ba31-570e905a81dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_sales</th>\n",
       "      <th>average_units_sold</th>\n",
       "      <th>max_sales</th>\n",
       "      <th>max_units_sold</th>\n",
       "      <th>population_variance_sales</th>\n",
       "      <th>population_variance_units_sold</th>\n",
       "      <th>sample_standard_deviation_sales</th>\n",
       "      <th>sample_standard_deviation_units_sold</th>\n",
       "      <th>sample_variance_units_sold</th>\n",
       "      <th>total_categories</th>\n",
       "      <th>total_departments</th>\n",
       "      <th>total_items</th>\n",
       "      <th>total_sales</th>\n",
       "      <th>total_states</th>\n",
       "      <th>total_stores</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>total_units_sold</th>\n",
       "      <th>day_over_day_units_sold</th>\n",
       "      <th>previous_days_units_sold</th>\n",
       "      <th>total_sales_30_prd_mv_avg</th>\n",
       "      <th>total_units_sold_28_day_max</th>\n",
       "      <th>total_units_sold_30_prd_mv_avg</th>\n",
       "      <th>date</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.960</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.96</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-2.131628e-15</td>\n",
       "      <td>36.45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.363961</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.60</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.60</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.980</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.98</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-5.329071e-16</td>\n",
       "      <td>35.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.236986</td>\n",
       "      <td>38.900000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.80</td>\n",
       "      <td>83.0</td>\n",
       "      <td>83</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.160</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.28</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.360000e-02</td>\n",
       "      <td>11.01</td>\n",
       "      <td>0.193218</td>\n",
       "      <td>3.497618</td>\n",
       "      <td>12.233333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.60</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.60</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.160</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.28</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.360000e-02</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0.193218</td>\n",
       "      <td>3.373096</td>\n",
       "      <td>11.377778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.60</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.60</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.112900e-02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.153221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.79</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   average_sales  average_units_sold  ...       date  predicted_class\n",
       "0          1.960                 7.5  ... 2015-01-01                1\n",
       "1          0.980                 8.3  ... 2015-01-01                1\n",
       "2          3.160                 4.3  ... 2015-01-01                1\n",
       "3          3.160                 4.4  ... 2015-01-01                1\n",
       "4          1.379                 0.0  ... 2015-01-01                1\n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d0c364-2bf9-469f-8b68-3e5096824e39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we want to convert our resulting dataframe back to spark so we can write our dataframe to a table using sparks write save as table function. We will construct our spark dataframe called classifed_table using a spark CreateDataFrame call. We needed to move our dataframe to a pandas dataframe becuase models can not take in a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b46153f6-90be-43cc-9037-f097bf944bbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "classifed_table = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e359fe-1fb0-40ad-9033-118bd36e62c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will write this table to a table in atscale_sample_data under the ai_link_ml_testing schema. If we wanted to write this table to any other data warehouse AI Link enables us to do that. Say for instance, your buisness team operates off of Snowflake but your data engineers work in Databricks, AI Link facilitates the movement of data across warehouses to ensure that your one source of truth it available to all. Lastly we have to set our mode to overwrite so we ensure we have the most up to date verison of our classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d82f00-a76e-406e-a551-0f4700e47892",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "classifed_table.write.saveAsTable(\"atscale_sample_data.ai_link_ml_testing.m5_model_item_class_predictions\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ef974b-d485-4394-a628-1c8e71d8b9c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create Webhook \n",
    "This will let our developers know when their model has been used to classify. To set up our webhooks we need to again run our set up notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ff1e44-0f34-42ec-acbd-6ae432d76667",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./06_Webhook_Set_Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f941f18-fe4c-42f2-8b09-da79bb37e57b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webhook Set Up Complete.\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09488aac-d1c4-43a1-9473-3df66f7536b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#format success message\n",
    "target_col = \"item type\"\n",
    "messages = [\"V--------------PREDICTION COMPLETE----------------V\"]\n",
    "messages += [\"MODEL NAME: \" + model_name + \" LATEST VERSION\"] \n",
    "messages += [\"DATE: \" + str(now)]\n",
    "messages += [\"PREDICTION TARGET: \" + target_col]\n",
    "messages += [\"MODEL URI: \" + model_uri]\n",
    "messages += [\"*******************************************\"]\n",
    "messages += [\" \"]\n",
    "messages += [\" \"]\n",
    "messages += [\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2708707d-1a20-4797-8e1f-f638ed323377",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "send_slack_messages(slack_url, channel, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13448a20-7705-4e9b-a56e-31add351891d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![](/files/images/atscale_logo.png)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04_Use_Model_To_Classify",
   "widgets": {}
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "name": "LightGBMClassifier-11301fae27476707c87e4675cbb2b6e3"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
