{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be8563b-66e1-4437-b6aa-8dbafeaf4db9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![](/files/images/dbxatscale.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eb2e2a3-dd31-4018-8472-763b8c94bf27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Overview\n",
    "In order to leverage the powers of Databricks' full AI/ML pipelining system, while using AtScale data, we need to move a small portion data to start our AutoML experiment. Databricks AutoML experiments require the data table to already exist in Databricks which our data does; however, it is spread out across two fact tables and all the desirable <b>Semantic Model</b> transformations are not actually enacted on those underlying data tables. In order to ensure our transformations are exposed to our AutoML experiment, we need to execute and write the result of our AI Link generated SQL query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8312f32a-373d-4685-8f63-1b8455d14ab4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Imports and AtScale connection: We need to create an AtScale connection in order to generate our database query which will define our new datatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6b2b5f8-210a-4abf-b2fe-7559f3275fe6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from atscale.client import Client\n",
    "from atscale.data_model import DataModel\n",
    "from atscale.project import Project\n",
    "from atscale.connection import Connection\n",
    "from atscale.eda.feature_engineering import *\n",
    "from atscale.base import enums\n",
    "from atscale.utils import db_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b81df22-6c4c-48aa-bdfc-00ce3eee0570",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Our username and password are stored in an Azure secret scope which we can access using AI Link's db.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3861e531-a116-4d4e-a7a5-65a75df9eeaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AILinkUser = dbutils.secrets.get(scope = \"ai-link scope\", key = \"atscale-andrew-user\")\n",
    "AILinkPass = dbutils.secrets.get(scope = \"ai-link scope\", key = \"atscale-andrew-pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c4350c2-8cd9-4570-8a3d-0b978d276dac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = Client(\n",
    "    server = \"http://ailink-public.atscale.com\",\n",
    "    organization = \"default\",\n",
    "    username = AILinkUser,\n",
    "    password = AILinkPass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b44a212-3f0c-4efc-8fe2-6e3e277b59d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d4488b-f265-4cfc-b18b-327e7811d4fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose a project:\nAutomatically selecting only option: \"ID: bfab4719-9a61-46f5-7d4c-1dcc2881ba44: Name: Walmart Databricks ML\"\nPlease choose a published project:\nAutomatically selecting only option: \"ID: 7f2140aa-8b8c-4517-6484-47432da7e337: Name: Walmart Databricks ML\"\nPlease choose a data model:\nAutomatically selecting only option: \"ID: 50fd61ba-2a0b-44d2-7dec-786e15c6ab6f: Name: m5_walmart_sales\"\n"
     ]
    }
   ],
   "source": [
    "project = client.select_project(name_contains=\"Walmart Databricks ML\")\n",
    "dm = project.select_data_model(name_contains= \"m5_walmart_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd4638b-aa2b-4406-85ce-948ed6bd8b93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For more information on the dataset behind this demo check it out <a href=\"https://www.kaggle.com/competitions/m5-forecasting-accuracy\">here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1558356-e454-486c-9d2f-17c90e350eef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Problem Statement\n",
    "Before we pull any data, let's discuss the model we are aiming to create, deploy, and use. For this demo, we are going to pretend our business team has asked us to create a model for predicting the type of an item based soley upon its sales information. So we want to create a model that sorts all of our sales into three categories: Food items, Hobby related items, and Household items. So we are going to pull all numerical sales related data, and our item so we can classify our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b61f67a-5ca3-49c9-b0f1-aac87116203a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using AI Link's get all calls we can retrieve a list of all categorical and all numeric features in our data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f8f6df-cb22-4eb0-8f2b-b8a4fd0df584",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catfeats = dm.get_all_categorical_feature_names()\n",
    "numfeats = dm.get_all_numeric_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871bc41d-90cd-4232-a311-af4314efa7d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We are then going to define our features as all of our numeric features as well as 3 categorical features appended onto the end. You can see our complete feature list printed below. Lastly, we build our SQL query for all of our features and save it as a string named query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcfff676-132f-4b55-9f71-576bb8a656ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average_sales', 'average_units_sold', 'max_sales', 'max_units_sold', 'population_variance_sales', 'population_variance_units_sold', 'sample_standard_deviation_sales', 'sample_standard_deviation_units_sold', 'sample_variance_units_sold', 'total_categories', 'total_departments', 'total_items', 'total_sales', 'total_states', 'total_stores', 'total_transactions', 'total_units_sold', 'day_over_day_units_sold', 'previous_days_units_sold', 'total_sales_30_prd_mv_avg', 'total_units_sold_28_day_max', 'total_units_sold_30_prd_mv_avg', 'date', 'store', 'item']\n"
     ]
    }
   ],
   "source": [
    "features = numfeats\n",
    "\n",
    "features.append(catfeats[2])\n",
    "features.append(catfeats[-1])\n",
    "features.append(catfeats[-3])\n",
    "\n",
    "print(features)\n",
    "query = dm.get_database_query(feature_list = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d4ba9f3-e118-4d91-b4b4-42c0cbda6380",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create our spark dataframe from our query. AI Link allows us to eliminate the need to move data through AtScale, instead we can programatically generate a dataframe based solely upon the underlying SQL generated by the AtScale engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8584497b-7f71-4ca4-bb71-dbfc75a4de5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0eb9104-06f8-4151-8ce5-ecd1e32df7c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now let's convert our data to pandas so we can divide up our 14 items into our 3 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20befa6b-0649-4d92-9abf-1e306967bf7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a53f10e3-e5f9-411b-9f4e-7760562fc1cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can now create a new column called item_type which consists of entirely 1s, 2s, or 3s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7257510e-3b11-4f55-92f7-c33cf7697381",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_sales</th>\n",
       "      <th>average_units_sold</th>\n",
       "      <th>max_sales</th>\n",
       "      <th>max_units_sold</th>\n",
       "      <th>population_variance_sales</th>\n",
       "      <th>population_variance_units_sold</th>\n",
       "      <th>sample_standard_deviation_sales</th>\n",
       "      <th>sample_standard_deviation_units_sold</th>\n",
       "      <th>sample_variance_units_sold</th>\n",
       "      <th>total_categories</th>\n",
       "      <th>total_departments</th>\n",
       "      <th>total_items</th>\n",
       "      <th>total_sales</th>\n",
       "      <th>total_states</th>\n",
       "      <th>total_stores</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>total_units_sold</th>\n",
       "      <th>day_over_day_units_sold</th>\n",
       "      <th>previous_days_units_sold</th>\n",
       "      <th>total_sales_30_prd_mv_avg</th>\n",
       "      <th>total_units_sold_28_day_max</th>\n",
       "      <th>total_units_sold_30_prd_mv_avg</th>\n",
       "      <th>item_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.96</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.96</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.96</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.98</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.28</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.28</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.28</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.28</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.28</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.28</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   average_sales  average_units_sold  ...  total_units_sold_30_prd_mv_avg  item_type\n",
       "0           1.96                19.0  ...                              19          1\n",
       "1           0.98                 9.0  ...                               9          1\n",
       "2           3.28                10.0  ...                              10          1\n",
       "3           3.28                 9.0  ...                               9          1\n",
       "4           1.48                 0.0  ...                               0          1\n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_cat = []\n",
    "\n",
    "foodc=0\n",
    "hobc=0\n",
    "housec=0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "  item = row[\"item\"][0:3]\n",
    "  if item == \"FOO\":\n",
    "    item_cat.append(1)\n",
    "    foodc+=1\n",
    "  elif item == \"HOB\":\n",
    "    item_cat.append(2)\n",
    "    hobc+=1\n",
    "  elif item ==\"HOU\":\n",
    "    item_cat.append(3)\n",
    "    housec+=1\n",
    "  \n",
    "df[\"item_type\"] = item_cat\n",
    "df = df.drop([\"item\", \"date\", \"store\"], axis = 1)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d54f1766-7398-48bf-882d-4afda8b8c5e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Write a table\n",
    "Finally let's just write our data as a table so we can use it as an AutoML input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44027660-e365-4365-a791-760a7d5722a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table = spark.createDataFrame(df)\n",
    "table.write.saveAsTable(\"hive_metastore.ai_link_data.m5_data_for_auto_ml\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fd1f553-2c50-4e2c-8aa7-371cfebab028",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![](/files/images/atscale_logo.png)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_AutoML_Setup",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
